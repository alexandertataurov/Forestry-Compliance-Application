#!/usr/bin/env python3
"""
Test All Workflow Configurations
Validates each workflow setup without launching Cursor instances.
"""

import time
import threading
from start import CursorWorkflowLauncher, WorkflowConfig

def test_workflow_configs():
    """Test all workflow configurations."""
    
    print("üß™ Testing All Workflow Configurations")
    print("=" * 50)
    
    workflows = {
        '2-agent': WorkflowConfig.get_2_agent_config(),
        '3-agent': WorkflowConfig.get_3_agent_config(), 
        '4-agent': WorkflowConfig.get_4_agent_config()
    }
    
    for workflow_name, config in workflows.items():
        print(f"\nüìã Testing {workflow_name.upper()} Workflow")
        print(f"Name: {config['name']}")
        print(f"Description: {config['description']}")
        print(f"Agents: {len(config['agents'])}")
        print(f"Channels: {len(config['channels'])}")
        
        # Validate agent configurations
        for agent_name, agent_config in config['agents'].items():
            print(f"  ‚úÖ {agent_name:12}: {agent_config['role']:12} "
                  f"({len(agent_config['channels'])} channels)")
            
            # Validate prompt has required elements
            prompt = agent_config['prompt']
            required_elements = ['ROLE:', 'register_agent', '[[END-OF-TURN]]']
            missing = [elem for elem in required_elements if elem not in prompt]
            
            if missing:
                print(f"    ‚ö†Ô∏è  Missing elements: {missing}")
            else:
                print(f"    ‚úÖ Prompt validation passed")
        
        # Validate channel consistency
        all_channels = set()
        for agent_config in config['agents'].values():
            all_channels.update(agent_config['channels'])
        
        config_channels = set(config['channels'])
        if all_channels == config_channels:
            print(f"  ‚úÖ Channel consistency validated")
        else:
            print(f"  ‚ö†Ô∏è  Channel mismatch: {all_channels - config_channels}")
    
    print(f"\nüéØ Workflow Feature Matrix:")
    print(f"{'Feature':<20} {'2-Agent':<10} {'3-Agent':<10} {'4-Agent':<10}")
    print("-" * 60)
    
    features = [
        ('Planning', '‚úÖ', '‚úÖ', '‚úÖ'),
        ('Execution', '‚úÖ', '‚úÖ', '‚úÖ'), 
        ('Code Review', '‚ùå', '‚úÖ', '‚úÖ'),
        ('GitHub PR', '‚ùå', '‚ùå', '‚úÖ'),
        ('Status Channel', '‚úÖ', '‚úÖ', '‚úÖ'),
        ('Loop Detection', '‚úÖ', '‚úÖ', '‚úÖ'),
        ('Session Monitoring', '‚úÖ', '‚úÖ', '‚úÖ')
    ]
    
    for feature, two, three, four in features:
        print(f"{feature:<20} {two:<10} {three:<10} {four:<10}")
    
    print(f"\n‚úÖ All workflow configurations validated!")
    return True

def test_launcher_integration():
    """Test launcher integration with mock setup."""
    
    print(f"\nüîß Testing Launcher Integration")
    print("-" * 30)
    
    # Test configuration loading
    launcher = CursorWorkflowLauncher()
    
    test_options = {
        'workspace': '/tmp/test-workspace',
        'timeout': 60,
        'max_iterations': 5, 
        'loop_threshold': 2,
        'gui': False,
        'log_level': 'DEBUG',
        'repo': False,
        'dry_run': False
    }
    
    # Test each workflow type
    for workflow_type in ['2-agent', '3-agent', '4-agent']:
        config = launcher._get_workflow_config(workflow_type)
        
        print(f"  üìã {workflow_type}: {config['name']}")
        print(f"    Agents: {len(config['agents'])}")
        print(f"    Channels: {len(config['channels'])}")
        
        # Validate configuration structure
        required_keys = ['name', 'description', 'channels', 'agents']
        missing_keys = [key for key in required_keys if key not in config]
        
        if missing_keys:
            print(f"    ‚ùå Missing keys: {missing_keys}")
        else:
            print(f"    ‚úÖ Configuration structure valid")
    
    print(f"\n‚úÖ Launcher integration tests passed!")
    return True

def test_prompt_quality():
    """Test prompt quality and completeness."""
    
    print(f"\nüìù Testing Prompt Quality")
    print("-" * 25)
    
    workflows = {
        '2-agent': WorkflowConfig.get_2_agent_config(),
        '3-agent': WorkflowConfig.get_3_agent_config(),
        '4-agent': WorkflowConfig.get_4_agent_config()
    }
    
    prompt_quality_checks = [
        ('Role Definition', lambda p: 'ROLE:' in p),
        ('MCP Registration', lambda p: 'register_agent' in p),
        ('Channel Usage', lambda p: any(ch in p for ch in ['a2e', 'e2a', 'r2g'])),
        ('Message Format', lambda p: any(fmt in p for fmt in ['{', 'FORMAT:', 'ENVELOPE:'])),
        ('Turn Ending', lambda p: '[[END-OF-TURN]]' in p),
        ('Workflow Steps', lambda p: any(word in p for word in ['WORKFLOW:', 'PROTOCOL:', 'STEPS:'])),
        ('Guidelines', lambda p: any(word in p for word in ['GUIDELINES:', 'RULES:', 'SAFETY:']))
    ]
    
    for workflow_name, config in workflows.items():
        print(f"\n  üìã {workflow_name.upper()} Prompts:")
        
        for agent_name, agent_config in config['agents'].items():
            prompt = agent_config['prompt']
            prompt_length = len(prompt)
            
            print(f"    ü§ñ {agent_name:12} ({prompt_length:4} chars)")
            
            passed_checks = 0
            for check_name, check_func in prompt_quality_checks:
                if check_func(prompt):
                    passed_checks += 1
                    print(f"      ‚úÖ {check_name}")
                else:
                    print(f"      ‚ö†Ô∏è  {check_name} - could be improved")
            
            quality_score = (passed_checks / len(prompt_quality_checks)) * 100
            print(f"      üìä Quality Score: {quality_score:.0f}%")
    
    print(f"\n‚úÖ Prompt quality analysis completed!")
    return True

def test_channel_communication():
    """Test channel communication patterns."""
    
    print(f"\nüì° Testing Channel Communication Patterns")
    print("-" * 40)
    
    # Communication flow analysis for each workflow
    flows = {
        '2-agent': [
            ('User', 'architect', 'Initial task request'),
            ('architect', 'a2e', 'Task breakdown'),
            ('executor', 'a2e', 'Pull task'),
            ('executor', 'e2a', 'Post results'),
            ('architect', 'e2a', 'Pull results'),
            ('architect', 'status', 'Update status')
        ],
        '3-agent': [
            ('architect', 'a2e', 'Task planning'),
            ('executor', 'e2r', 'Implementation results'),
            ('reviewer', 'r2a', 'Review feedback'),
            ('architect', 'status', 'Workflow coordination')
        ],
        '4-agent': [
            ('architect', 'a2e', 'Task planning'),
            ('executor', 'e2r', 'Implementation'),
            ('reviewer', 'r2g', 'Approved changes'),
            ('github', 'g2a', 'PR status'),
            ('github', 'status', 'Git operations')
        ]
    }
    
    for workflow_name, flow in flows.items():
        print(f"\n  üìã {workflow_name.upper()} Communication Flow:")
        
        for i, (source, target, description) in enumerate(flow, 1):
            print(f"    {i}. {source:12} ‚Üí {target:12} : {description}")
        
        # Validate no circular dependencies  
        sources = [f[0] for f in flow]
        targets = [f[1] for f in flow]
        
        if len(set(sources + targets)) == len(sources + targets):
            print(f"    ‚úÖ No circular dependencies detected")
        else:
            print(f"    ‚ö†Ô∏è  Potential circular dependency")
    
    print(f"\n‚úÖ Channel communication analysis completed!")
    return True

def main():
    """Run all workflow tests."""
    
    print("üî¨ Comprehensive Workflow Testing Suite")
    print("=" * 60)
    
    tests = [
        ("Configuration Validation", test_workflow_configs),
        ("Launcher Integration", test_launcher_integration), 
        ("Prompt Quality Analysis", test_prompt_quality),
        ("Channel Communication", test_channel_communication)
    ]
    
    passed_tests = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nüß™ Running: {test_name}")
        try:
            success = test_func()
            if success:
                passed_tests += 1
                print(f"‚úÖ {test_name} PASSED")
            else:
                print(f"‚ùå {test_name} FAILED")
        except Exception as e:
            print(f"üí• {test_name} ERROR: {e}")
    
    print(f"\nüéØ Test Results Summary")
    print("=" * 30)
    print(f"Passed: {passed_tests}/{total_tests}")
    print(f"Success Rate: {(passed_tests/total_tests)*100:.0f}%")
    
    if passed_tests == total_tests:
        print(f"\nüéâ All tests passed! Workflows are ready for production.")
        
        print(f"\nüí° Next Steps:")
        print(f"  1. Install GUI dependencies: sudo apt install xdotool xclip")
        print(f"  2. Test basic workflow: python start.py --dry-run")
        print(f"  3. Start 2-agent workflow: python start.py")
        print(f"  4. Scale up as needed: python start.py 3-agent")
        
    else:
        print(f"\n‚ö†Ô∏è  Some tests failed. Review output above for issues.")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    main()